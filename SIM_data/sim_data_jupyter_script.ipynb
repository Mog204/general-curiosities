{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32b46071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Monday 11 November 2025\n",
    "\n",
    "@author: Imogen S\n",
    "\n",
    "---- Description ----\n",
    "This script reads default Simulation files created for Cabrini Hospital, and logs them into a searchable database based on key features.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Path for sim files, including any grouped folders\n",
    "path_sim_files = Path(\"C:/Users/imy1/Documents/Work/Mama/Clean SIM data\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "import ntpath\n",
    "import collections\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# optional: support .doc via textract (requires installation and native deps)\n",
    "try:\n",
    "    import textract\n",
    "    HAS_TEXTRACT = True\n",
    "except Exception:\n",
    "    HAS_TEXTRACT = False\n",
    "\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "\n",
    "def getMetaData(doc):\n",
    "    metadata = {}\n",
    "    prop = doc.core_properties\n",
    "    metadata[\"author\"] = prop.author\n",
    "    metadata[\"category\"] = prop.category\n",
    "    metadata[\"comments\"] = prop.comments\n",
    "    metadata[\"content_status\"] = prop.content_status\n",
    "    metadata[\"created\"] = prop.created\n",
    "    metadata[\"identifier\"] = prop.identifier\n",
    "    metadata[\"keywords\"] = prop.keywords\n",
    "    metadata[\"last_modified_by\"] = prop.last_modified_by\n",
    "    metadata[\"language\"] = prop.language\n",
    "    metadata[\"modified\"] = prop.modified\n",
    "    metadata[\"subject\"] = prop.subject\n",
    "    metadata[\"title\"] = prop.title\n",
    "    metadata[\"version\"] = prop.version\n",
    "    return metadata\n",
    "\n",
    "def get_title(doc_path, fname):\n",
    "    document = Document(doc_path)\n",
    "    all_strips = []\n",
    "    index = None\n",
    "    for i,para in enumerate(document.paragraphs):\n",
    "        searchtext = para.text.lower().strip()\n",
    "        all_strips.append(searchtext)\n",
    "        if 'title' in searchtext:\n",
    "            index = i\n",
    "            break\n",
    "    if index != None:\n",
    "        if all_strips[index].replace(\"title\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            title_text = all_strips[index].replace(\"title\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        else:\n",
    "            title_text = all_strips[index+1].replace(\"title\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        return title_text, 0\n",
    "    else:\n",
    "        return fname, 1\n",
    "    \n",
    "def get_date(doc_path, meta_date):\n",
    "    document = Document(doc_path)\n",
    "    all_strips = []\n",
    "    index = None\n",
    "    for i,para in enumerate(document.paragraphs):\n",
    "        searchtext = para.text.lower().strip()\n",
    "        all_strips.append(searchtext)\n",
    "        if findWholeWord('date')(searchtext)  and i<50 :\n",
    "            index = i\n",
    "            break\n",
    "    if index != None:\n",
    "        if all_strips[index].replace(\"date\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_date = all_strips[index].replace(\"date\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        elif all_strips[index+1].replace(\"date\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_date = all_strips[index+1].replace(\"date\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        else:\n",
    "            search_date = meta_date\n",
    "        \n",
    "        # Parse the input string into a datetime object\n",
    "        # %d for day, %m for month, %y for two-digit year\n",
    "\n",
    "        if search_date[-4:].isnumeric() == False:\n",
    "                search_date = search_date[:-2] + \"20\" + search_date[-2:]\n",
    "\n",
    "        if '/' not in search_date: #search_date.replace(\" \",\"\").isalpha() == True:\n",
    "            search_date = meta_date.strftime(\"%d/%m/%Y\") if meta_date != None else None\n",
    "        \n",
    "        date_object = datetime.strptime(search_date, \"%d/%m/%Y\")\n",
    "\n",
    "        # %Y for four-digit year\n",
    "        search_date_yyyy = date_object.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        return search_date_yyyy, 0\n",
    "\n",
    "    else:\n",
    "        fixed_meta_date = meta_date.strftime(\"%d/%m/%Y\") if meta_date != None else None\n",
    "        return fixed_meta_date, 1\n",
    "    \n",
    "def get_author(doc_path, meta_author):\n",
    "    document = Document(doc_path)\n",
    "    all_strips = []\n",
    "    index = None\n",
    "    for i,para in enumerate(document.paragraphs):\n",
    "        searchtext = para.text.lower().strip()\n",
    "        all_strips.append(searchtext)\n",
    "        if findWholeWord('author')(searchtext)  and i<50 :\n",
    "            index = i\n",
    "            break\n",
    "    if index != None:\n",
    "        if all_strips[index].replace(\"author\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_author = all_strips[index].replace(\"author\",\"\").replace(\"*\",\"\").replace(\":\",\"\").strip()\n",
    "        elif all_strips[index+1].replace(\"author\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_author = all_strips[index+1].replace(\"author\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        else:\n",
    "            search_author = meta_author\n",
    "        return search_author, 0\n",
    "    else:\n",
    "        if meta_author == 'python-docx':\n",
    "            return None, 1\n",
    "        return meta_author, 1\n",
    "    \n",
    "def get_target(doc_path):\n",
    "    document = Document(doc_path)\n",
    "    all_strips = []\n",
    "    index = None\n",
    "    for i,para in enumerate(document.paragraphs):\n",
    "        searchtext = para.text.lower().strip()\n",
    "        all_strips.append(searchtext)\n",
    "        if findWholeWord('target')(searchtext) or findWholeWord('audience')(searchtext) or 'players' in searchtext \\\n",
    "            or findWholeWord('developed for')(searchtext) and i<50 :\n",
    "            index = i\n",
    "            break\n",
    "    if index != None:\n",
    "        if all_strips[index].replace(\"target\",\"\").replace(\"audience\",\"\").replace(\"intended\",\"\").replace(\"players\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_target = all_strips[index].replace(\"target\",\"\").replace(\"audience\",\"\").replace(\"intended\",\"\").replace(\"players\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        elif all_strips[index+1].replace(\"target\",\"\").replace(\"audience\",\"\").replace(\"intended\",\"\").replace(\"players\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_target = all_strips[index+1].replace(\"target\",\"\").replace(\"audience\",\"\").replace(\"intended\",\"\").replace(\"players\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        else:\n",
    "            search_target = None\n",
    "        if findWholeWord('role')(search_target) or len(search_target) >100:\n",
    "            search_target = None\n",
    "        return search_target, 0\n",
    "    else:\n",
    "        return None, 1\n",
    "    \n",
    "\n",
    "# Objectives\n",
    "def get_objectives(doc_path):\n",
    "    document = Document(doc_path)\n",
    "    all_strips = []\n",
    "    index = None\n",
    "    for i,para in enumerate(document.paragraphs):\n",
    "        searchtext = para.text.lower().strip()\n",
    "        all_strips.append(searchtext)\n",
    "        if 'objective' in searchtext or 'goal' in searchtext:\n",
    "            index = i+1\n",
    "            test_ind = index\n",
    "            while document.paragraphs[test_ind].text.lower().strip() != \"\" and test_ind < len(document.paragraphs)-1:\n",
    "                test_ind +=1\n",
    "            \n",
    "            end_index = test_ind\n",
    "            break\n",
    "    if index != None:\n",
    "        extracted_text = [document.paragraphs[i].text.lower().strip() for i in range(index, end_index)]\n",
    "        search_obj = \"\\n\".join(extracted_text)\n",
    "        if len(search_obj) > 1000:\n",
    "            search_obj = None\n",
    "        return search_obj, 0\n",
    "    else:\n",
    "        return None, 1\n",
    "\n",
    "# Reviewer\n",
    "def get_reviewer(doc_path):\n",
    "    document = Document(doc_path)\n",
    "    all_strips = []\n",
    "    index = None\n",
    "    for i,para in enumerate(document.paragraphs):\n",
    "        searchtext = para.text.lower().strip()\n",
    "        all_strips.append(searchtext)\n",
    "        if findWholeWord('reviewer')(searchtext):\n",
    "            index = i+1\n",
    "            break\n",
    "    if index != None:\n",
    "        if all_strips[index].replace(\"reviewer\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_reviewer = all_strips[index].replace(\"reviewer\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        elif all_strips[index+1].replace(\"reviewer\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_reviewer = all_strips[index+1].replace(\"reviewer\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        else:\n",
    "            search_reviewer = None\n",
    "        return search_reviewer, 0\n",
    "    else:\n",
    "        return None, 1\n",
    "    \n",
    "# Difficulty\n",
    "def get_difficulty(doc_path):\n",
    "    document = Document(doc_path)\n",
    "    all_strips = []\n",
    "    index = None\n",
    "    for i,para in enumerate(document.paragraphs):\n",
    "        searchtext = para.text.lower().strip()\n",
    "        all_strips.append(searchtext)\n",
    "        if 'difficulty' in searchtext:\n",
    "            index = i\n",
    "            break\n",
    "    if index != None:\n",
    "        if all_strips[index].replace(\"difficulty\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_difficulty = all_strips[index].replace(\"difficulty\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        elif all_strips[index+1].replace(\"difficulty\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip() != \"\":\n",
    "            search_difficulty = all_strips[index+1].replace(\"difficulty\",\"\").replace(\":\",\"\").replace(\"*\",\"\").strip()\n",
    "        else:\n",
    "            search_difficulty = None\n",
    "\n",
    "        if '/' not in search_difficulty or len(search_difficulty) >1000:\n",
    "            search_difficulty = None\n",
    "        return search_difficulty, 0\n",
    "    else:\n",
    "        return None, 1\n",
    "\n",
    "def extract_titles(root_dir, output_dir=None, save_csv=True):\n",
    "    sim_titles = []\n",
    "    types = []\n",
    "    rows=[]\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            file_title = os.path.splitext(fname)[0]\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext in (\".docx\", \".doc\"):\n",
    "                try:\n",
    "                    src_path = os.path.join(dirpath, fname)\n",
    "                    sim_titles.append(get_title(src_path, file_title)[0])\n",
    "                    types.append(get_title(src_path, file_title)[1])\n",
    "                except Exception as e:\n",
    "                    # record failure and continue\n",
    "                    rows.append({\"source_path\": src_path, \"status\": \"error\", \"message\": str(e), \"output_path\": None})\n",
    "                    continue\n",
    "    return sim_titles, types, rows\n",
    "\n",
    "sim_titles_test, types, rows = extract_titles(path_sim_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89962c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "\n",
    "def extract_pdf_to_word(pdf_path, word_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and saves it into a Word document.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the input PDF file.\n",
    "        word_path (str): The path to the output Word document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the PDF document\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "        # Create a new Word document\n",
    "        word_document = Document()\n",
    "\n",
    "        # Iterate through each page of the PDF\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text = page.get_text(\"text\")  # Extract text from the page\n",
    "\n",
    "            # Add the extracted text as a paragraph to the Word document\n",
    "            word_document.add_paragraph(text)\n",
    "\n",
    "        # Save the Word document\n",
    "        word_document.save(word_path)\n",
    "        print(f\"Text successfully extracted from '{pdf_path}' to '{word_path}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def convert_to_docs(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext == \".pdf\":\n",
    "                try:\n",
    "                    src_path = os.path.join(dirpath, fname)\n",
    "                    base_name = os.path.splitext(fname)[0]\n",
    "                    output_path = os.path.join(dirpath, base_name + \".docx\")\n",
    "                    extract_pdf_to_word(src_path, output_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing '{src_path}': {e}\")\n",
    "convert_to_docs(path_sim_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5d3bb063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source        C:\\Users\\imy1\\Documents\\Work\\Mama\\Clean SIM da...\n",
      "Title         simulation gamification / live.die.repeat  ecl...\n",
      "Author                                                m bennett\n",
      "Date                                                 02/04/2025\n",
      "Attendees                                ed registars ts 1 -> 4\n",
      "Reviewer                                                   None\n",
      "Objectives    primary:\\nprogression through 3-6 levels of re...\n",
      "Difficulty                                                 None\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features = [\"Source\", \"Title\", \"Author\", \"Date\", \"Attendees\", \"Reviewer\", \"Objectives\", \"Difficulty\"]\n",
    "def extract_info(root_dir, output_dir=None, save_csv=True):\n",
    "    sim_titles = []\n",
    "    types = []\n",
    "    rows=[]\n",
    "    \n",
    "\n",
    "    len_docs = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            file_title = os.path.splitext(fname)[0]\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext in (\".docx\", \".doc\"):\n",
    "                len_docs += 1\n",
    "\n",
    "    \n",
    "    info_df = pd.DataFrame(index=range(len_docs),columns = features)\n",
    "    \n",
    "    ind = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            file_title = os.path.splitext(fname)[0]\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext in (\".docx\", \".doc\"):\n",
    "                try:\n",
    "                    src_path = os.path.join(dirpath, fname)\n",
    "                    doc = Document(src_path)\n",
    "\n",
    "                    # Extract source path\n",
    "                    info_df.at[ind, \"Source\"] = src_path\n",
    "\n",
    "                    # Extract title\n",
    "                    info_df.at[ind,\"Title\"] = get_title(src_path, file_title)[0]\n",
    "                    ##sim_titles.append(get_title(src_path, file_title)[0])\n",
    "                    ##types.append(get_title(src_path, file_title)[1])\n",
    "\n",
    "                    # Extract metadata\n",
    "                    metadata_dict = getMetaData(doc)\n",
    "\n",
    "\n",
    "                    # Extract date\n",
    "                    info_df.at[ind,\"Date\"]= get_date(src_path, metadata_dict[\"created\"])[0]\n",
    "\n",
    "                    # Extract author\n",
    "                    info_df.at[ind,\"Author\"]= get_author(src_path, metadata_dict[\"author\"])[0]\n",
    "                    \n",
    "                    # Extract Target / attendees\n",
    "                    info_df.at[ind,\"Attendees\"]= get_target(src_path)[0]\n",
    "\n",
    "                    # Extract Objectives\n",
    "                    info_df.at[ind,\"Objectives\"]= get_objectives(src_path)[0]\n",
    "\n",
    "                    # Extract Reviewer\n",
    "                    info_df.at[ind,\"Reviewer\"]= get_reviewer(src_path)[0]\n",
    "\n",
    "                    # Extract Difficulty\n",
    "                    info_df.at[ind,\"Difficulty\"]= get_difficulty(src_path)[0]\n",
    "\n",
    "                    ind += 1\n",
    "                    ##dates.append(metadata_dict[\"created\"])\n",
    "\n",
    "\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # record failure and continue\n",
    "                    rows.append({\"source_path\": src_path, \"status\": \"error\", \"message\": str(e), \"output_path\": None})\n",
    "                    continue\n",
    "    return info_df\n",
    "\n",
    "info_df_test = extract_info(path_sim_files)\n",
    "print(info_df_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5708f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1:H200\n"
     ]
    }
   ],
   "source": [
    "# save to searchable table in excel\n",
    "\n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "\n",
    "# create directory for excel\n",
    "directory_path = path_sim_files / \"Excel summaries\"\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "def get_letter_from_position(position, uppercase=False):\n",
    "    if uppercase:\n",
    "        # 'A' is Unicode 65, so add (position - 1) to 65\n",
    "        return chr(65 + (position - 1))\n",
    "    else:\n",
    "        # 'a' is Unicode 97, so add (position - 1) to 97\n",
    "        return chr(97 + (position - 1))\n",
    "\n",
    "writer = pd.ExcelWriter(Path(directory_path / f'{datetime.strftime(datetime.today(), '%Y-%m-%d')} SIM table.xlsx'), engine='xlsxwriter')\n",
    "\n",
    "\n",
    "# 3. Write the DataFrame data to XlsxWriter\n",
    "info_df_test.to_excel(writer, sheet_name='Sims', index=False)\n",
    "\n",
    "\n",
    "# 4. Get the xlsxwriter workbook and worksheet objects\n",
    "workbook = writer.book\n",
    "worksheet = writer.sheets['Sims']\n",
    "\n",
    "# Add a format for text wrapping\n",
    "wrap_format = workbook.add_format({'text_wrap': True})\n",
    "\n",
    "# 5. Add the Excel table structure\n",
    "# The range corresponds to the data in the DataFrame (3 columns, 3 rows + header)\n",
    "end_letter = get_letter_from_position(len(info_df_test.columns), uppercase=True)\n",
    "end_row = len(info_df_test) + 1  # +1 for header row\n",
    "table_range = f'A1:{end_letter}{end_row}'\n",
    "\n",
    "print(table_range)\n",
    "\n",
    "columns_dict = [{'header': col} for col in info_df_test.columns]\n",
    "\n",
    "worksheet.add_table(table_range, {'columns': columns_dict})\n",
    "\n",
    "for col_num, value in enumerate(info_df_test.columns.values):\n",
    "    worksheet.set_column(col_num, col_num, 30, wrap_format)\n",
    "# 6. Close the Pandas Excel writer and output the Excel file\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cedb68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMUPdf\n",
      "  Downloading pymupdf-1.26.6-cp310-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.6-cp310-abi3-win_amd64.whl (18.4 MB)\n",
      "   ---------------------------------------- 0.0/18.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.6/18.4 MB 7.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.7/18.4 MB 7.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 5.2/18.4 MB 7.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 6.8/18.4 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 8.4/18.4 MB 7.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 10.0/18.4 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 11.5/18.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 13.1/18.4 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.4/18.4 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 15.7/18.4 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.3/18.4 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.4/18.4 MB 7.0 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMUPdf\n",
      "Successfully installed PyMUPdf-1.26.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d4976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
